## ðŸ› ï¸ TODO â€“ Upcoming Improvements
- [ ] identificirat potencijalne bottleneckove i bugove i popravit
- [ ] napravit cleaning na nodesima malo
- [ ] Bolji `try/except` handling u nodu i serveru (posebno mreÅ¾ni pozivi)
- [ ] Provat deployat server na dockerhub
- [ ] CLI argumenti u `node.py` (`--node_id`, `--server`, `--cam_index`, itd.)
- âŒ Failover mehanizam: Äuvanje embeddinga offline ako je server nedostupan - > dali ovo ima smisla? Koji Ä‡e mi to k realno? Server pukne i onda kad se nazad upali, klasificira osobu koja je pred po ure bila pred kameron...useless
- [ ] Test skripte za sve rute + `pytest` test suite
- [ ] DinamiÄko skaliranje nodeova â€“ svaki node lokalno prati aktivnost (npr. broj lica ili kretanja) i, ako detektira neaktivnost kroz odreÄ‘eno vrijeme, automatski se prebacuje u *idle mode* (pauzira model i obradu); Äim ponovno otkrije aktivnost, reaktivira se za punu obradu
- [ ] (Future) API token autentikacija za sigurnost komunikacije - > dodat neki credential u node i onda kad Å¡alje neÅ¡to na server, server brzinski provjeri dali request sadrÅ¾i taj credential (vidit dali da stavljan JWT ili neÅ¡to jednostavnije) - > poÅ¡to je redis middleware, ta provjera se svakako odvija unitar classify worker jer on vadi iz redisa. Server ima listu approved tokena i brzinski provjerava dali je request poslan s validnog nodea => ovo fixat; ona varijanta s jsonima ni bila nikako dobra....jwt it is after all
Definitivno svaki node ima unique credentials...nema smisla da postoji neki common

Ovo gore je ok za poÄetak, ali:

Taj security se svodi na 2 Äitanja iz enva. Bolje da sloÅ¾imo da se kreira jwt pri paljenju nodea i Å¡alje se serveru pri initial healthchecku. Server lipo ima listu allowed tokena i to brzinski provjerava svaki put kad dobije request



Bitna ideja: - >Ã—kad to napravin, redoat docker compose da server povuÄe promjene

ðŸŒ Global Time Sync TODO

Sve u UTC

Node odmah pretvori svoj lokalni timestamp u UTC.

Na serveru NIKAD ne koristi lokalno vrijeme â†’ sve se sprema u UTC.

Server Å¡alje heartbeat (svakih X sekundi)

Server Å¡alje: current_utc_time.

Node izraÄuna koliko mu kasni ili Å¾uri sat (drift_offset).

Node korigira svoj clock

Node NE dira sistemski sat.

Samo kod slanja eventa dodaje ili oduzima drift_offset na UTC timestamp.

Event struktura (koja se Å¡alje serveru)

event_id (unikatni ID ili counter).

utc_timestamp (vrijeme u UTC).

local_time (vrijeme u zoni noda, samo za info/debug).

timezone_offset (koliko sati/minuta iza/ahead UTC).

drift_offset (koliko smo morali ispraviti clock).

corrected_utc (finalno vrijeme koje se koristi).

Server sve sprema po corrected_utc

Ovo je jedini timestamp koji ulazi u logiku attendancea i alertinga.

Prikaz korisniku

Kod reporta/analitike â†’ frontend konvertira corrected_utc u lokalno vrijeme korisnika (ovisno o njihovoj zoni).

Fallback za ordering (Lamport clock)

Svaki node ima counter koji se inkrementira za svaki event.

Dodaj lamport_clock u event payload.

Ako doÄ‘e do clock problema, server sloÅ¾i redoslijed eventa po tom counteru.

Audit / Debug

ÄŒuvaj sve: originalni local_time, utc_timestamp, drift_offset, corrected_utc.

To ti pomaÅ¾e kad neko kaÅ¾e â€œali ja sam stigao na posao u 9h!!â€ pa moÅ¾eÅ¡ dokazati drift.

- âœ… ~~Napravit kompletnu pipeline schemu od poÄetka do kraja procesa~~
- âœ… ~~Implementirat counter per node~~
- âœ… ~~Provat primjenit segmentaciju i na loading poznatih lici - > might boost precision~~  https://github.com/AntonioLabinjan/Image-Segmentator
- âœ… ~~Flask->FastAPI migracija za server; na nodesima ne triba I think~~
- âœ… ~~Roknut kod od nodesa na eng~~
- âœ…[~~idealno Ä‡e bit spremit server url u env pa ga vadit iz enva...pridonosi skalabilnosti jer onda lakÅ¡e dodamo novi node bez da imamo pojma di se server vrti(ZAJEB...ne spreman server url nikamo jer je redis middleware izmeÄ‘u nodesa i servera) - > u env Ä‡emo stavit: threshold distance i threshold time iz nodesa i app.run paramse iz server.py~~
- âœ… ~~Centralizirat FPS/Latency monitor za sve nodese (sve metrike na 1 mistu) => better; posebni log file za svaki node...nema smisla da se loga u 1 file, pa da triban scrollat ko Å¡tupido...praktiÄnije je ovako~~
- âœ… ~~Add latency logging on all nodes~~
- âœ… ~~Fallback mehanika.za nodes...neki lifesaver ako node crkne - > probbably neki health check unutar nodea~~
- âœ… ~~Zamjena `print()` s `logging` modulom + log fajlovi za greÅ¡ke i info~~
- âœ… ~~Dodati `/ping` i `/heartbeat` rute za health monitoring servera i nodova~~
- âœ… ~~Async ili threaded slanje embeddinga za niÅ¾u latenciju (BAD IDEA; EVENT BASED SENDING AKO SU EMBEDDINZI DOVOLJNO RAZLIÄŒITI)~~
- âœ… ~~Live dashboard `/nodes` za pregled statusa svih nodova~~
- âœ… ~~Automatski refresh dataseta~~
- âœ… ~~Better event based features; napravit da ne spamma konstantno isti embedding nego nekako viÅ¡e graceful->implement should_classify fn~~
- âœ… ~~Find alternative for regular Python queueÃ—(redis probbably)~~
- âœ… ~~Ne zabit da se redis mora vrtit u dockeru~~
- âœ… ~~Malo poradit na modularnosti i orkestraciji (npr. globalni imports file, centralni runner za nodese di samo prosljedin idjeve koje Å¾elin upalit i sl.)~~
- âœ… ~~Implementirat upozorenje ako je env premraÄan~~
- âœ… ~~Snapshot spremanje slike prilikom slanja embeddinga (debug/dataset) => NI SLUÄŒAJNO OVO IMPLEMENTIRAT; SLIKE DETEKCIJE SE NE POHRANJUJU!!!!!~~
- âœ… ~~Implement FPS/Latency tracking~~
- âœ… ~~Poigrat se malo s dockeron i njegovima moguÄ‡nostima sad kad je i novi server gore~~
-  âœ…~~napravit da bbox zazeleni kad prepozna/pocrveni kad ne prepozna~~
